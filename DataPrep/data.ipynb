{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Path to the folder containing Excel files\n",
    "folder_path = '/Users/soumilhooda/Desktop/Data-WD/Renewables-Messy'\n",
    "\n",
    "# Path to the folder where CSV files will be saved\n",
    "output_folder = '/Users/soumilhooda/Desktop/Data-WD/Renewables-Clean'\n",
    "\n",
    "# Define a function to extract English version after '/'\n",
    "def extract_english_version(state):\n",
    "    parts = state.split('/')\n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()  # Return the English version, stripped of leading/trailing whitespace\n",
    "    else:\n",
    "        return state.strip()  # Return the original state if no '/' found\n",
    "    \n",
    "# Function to process each Excel file\n",
    "def process_excel_file(file_path):\n",
    "    try:\n",
    "        # Read Excel file with skiprows set to 4\n",
    "        df = pd.read_excel(file_path, skiprows=4)\n",
    "\n",
    "        # Extract year from the date in the third column\n",
    "        year = pd.to_datetime(df.columns[2]).year\n",
    "\n",
    "        # Extract relevant data\n",
    "        relevant_data = df.iloc[1:42, 1:6]\n",
    "\n",
    "        # Rename columns\n",
    "        relevant_data.columns = ['State', 'Wind Energy', 'Solar Energy', 'RES', 'Total']\n",
    "\n",
    "        # Apply the function to the 'State' column\n",
    "        relevant_data['State'] = relevant_data['State'].apply(extract_english_version)\n",
    "\n",
    "        # Generate output file name (YYYY-MM-DD.csv)\n",
    "        date_str = df.columns[2]  # Assuming df.columns[2] contains the date string like \"12 Jan 2022\"\n",
    "        output_file_name = os.path.join(output_folder, f\"{year:04d}-{pd.to_datetime(date_str).strftime('%m-%d')}.csv\")\n",
    "\n",
    "        # Save relevant data to CSV\n",
    "        relevant_data.to_csv(output_file_name, index=False)\n",
    "\n",
    "        # print(f\"Successfully rewrote {file_path} to {output_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}. Error: {str(e)}\")\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.xlsx') or file_name.endswith('.xls'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        process_excel_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing input CSV files\n",
    "input_folder = '/Users/soumilhooda/Desktop/Data-WD/Renewables/Renewables-Clean'\n",
    "\n",
    "# Path to the folder where output CSV files will be saved\n",
    "output_folder = '/Users/soumilhooda/Desktop/Data-WD/Renewables/Renewables-FormWise'\n",
    "\n",
    "# List of Indian states\n",
    "indian_states = [\n",
    "    'Chandigarh', 'Delhi', 'Haryana', 'Himachal Pradesh', 'Jammu & Kashmir', 'Ladakh', 'Punjab', 'Rajasthan',\n",
    "    'Uttar Pradesh', 'Uttarakhand', 'Northern Region', 'Chhattisgarh', 'Gujarat', 'Madhya Pradesh', 'Maharashtra',\n",
    "    'Daman & Diu', 'Dadra & Nagar Haveli', 'Goa', 'Western Region', 'Andhra Pradesh', 'Telangana', 'Karnataka',\n",
    "    'Kerala', 'Tamil Nadu', 'Puducherry', 'Southern Region', 'Bihar', 'Jharkhand', 'Odisha', 'West Bengal',\n",
    "    'Sikkim', 'Eastern Region', 'Arunachal Pradesh', 'Assam', 'Manipur', 'Meghalaya', 'Mizoram', 'Nagaland',\n",
    "    'Tripura', 'North-Eastern Region', 'All India'\n",
    "]\n",
    "\n",
    "# Create empty DataFrames for Wind Energy, Solar Energy, RES, and Total Renewable Energy\n",
    "wind_energy_df = pd.DataFrame(index=pd.date_range(start='2021-01-01', end='2023-12-31'), columns=indian_states)\n",
    "solar_energy_df = pd.DataFrame(index=pd.date_range(start='2021-01-01', end='2023-12-31'), columns=indian_states)\n",
    "res_df = pd.DataFrame(index=pd.date_range(start='2021-01-01', end='2023-12-31'), columns=indian_states)\n",
    "total_renewable_energy_df = pd.DataFrame(index=pd.date_range(start='2021-01-01', end='2023-12-31'), columns=indian_states)\n",
    "\n",
    "# Iterate through each input CSV file\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Extract date from the file name\n",
    "        date_str = file_name.split('.')[0]\n",
    "\n",
    "        # Iterate through each state and fill the respective DataFrame\n",
    "        for state in indian_states:\n",
    "            state_data = df[df['State'] == state]\n",
    "            if not state_data.empty:\n",
    "                wind_energy_df.loc[date_str, state] = state_data['Wind Energy'].values[0]\n",
    "                solar_energy_df.loc[date_str, state] = state_data['Solar Energy'].values[0]\n",
    "                res_df.loc[date_str, state] = state_data['RES'].values[0]\n",
    "                total_renewable_energy_df.loc[date_str, state] = state_data['Total'].values[0]\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Processed data for file: {file_name}\")\n",
    "\n",
    "# Save DataFrames to separate output CSV files\n",
    "wind_energy_df.to_csv(os.path.join(output_folder, 'WindEnergy.csv'))\n",
    "solar_energy_df.to_csv(os.path.join(output_folder, 'SolarEnergy.csv'))\n",
    "res_df.to_csv(os.path.join(output_folder, 'RES.csv'))\n",
    "total_renewable_energy_df.to_csv(os.path.join(output_folder, 'Total.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing power stocks CSV files\n",
    "folder_path = '/Users/soumilhooda/Desktop/WD/Data-WD/Energy Stocks'\n",
    "\n",
    "# List to store dataframes for each stock\n",
    "stock_dfs = []\n",
    "\n",
    "# Iterate through each CSV file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        stock_name = file_name.split('.')[0]  # Extract stock name from the filename\n",
    "        \n",
    "        try:\n",
    "            # Read CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['Date'] = pd.to_datetime(df['Date'])  # Convert 'Date' column to datetime\n",
    "            df.set_index('Date', inplace=True)  # Set 'Date' column as index\n",
    "            \n",
    "            # Calculate average of 'High' and 'Low' columns\n",
    "            df[f'AVG_{stock_name}'] = (df['High'] + df['Low']) / 2\n",
    "            \n",
    "            # Rename 'Volume' column to include stock name\n",
    "            df.rename(columns={'Volume': f'Volume_{stock_name}'}, inplace=True)\n",
    "            \n",
    "            # Add dataframe to the list\n",
    "            stock_dfs.append(df[[f'AVG_{stock_name}', f'Volume_{stock_name}']])\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Processed data for file: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process data for file: {file_name}, Error: {e}\")\n",
    "\n",
    "# Create a dataframe indexed by dates from 01-01-1997 to 31-12-2023\n",
    "date_range = pd.date_range(start='1997-01-01', end='2023-12-31')\n",
    "overall_df = pd.DataFrame(index=date_range)\n",
    "\n",
    "# Merge dataframes for all stocks\n",
    "for df in stock_dfs:\n",
    "    overall_df = overall_df.merge(df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Fill NULL values with 'NULL'\n",
    "overall_df.fillna('NULL', inplace=True)\n",
    "\n",
    "# Save merged dataframe to CSV file\n",
    "overall_df.to_csv(os.path.join(folder_path, 'Overall_PowerStocks.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Electricity1 and Electricity2 CSV files\n",
    "electricity1 = pd.read_csv('/Users/soumilhooda/Desktop/WD/Data-WD/Electricity/Electricity1.csv')\n",
    "electricity2 = pd.read_csv('/Users/soumilhooda/Desktop/WD/Data-WD/Electricity/Electricity2.csv')\n",
    "\n",
    "# Convert date columns to consistent format (YYYY-MM-DD)\n",
    "electricity1['Date'] = pd.to_datetime(electricity1['Date'], format='%d-%m-%Y')\n",
    "electricity2['Date'] = pd.to_datetime(electricity2['Date'], format='%m/%d/%y')\n",
    "\n",
    "# Fill 0 values with NULL\n",
    "electricity1 = electricity1.replace(0, pd.NA)\n",
    "electricity2 = electricity2.replace(0, pd.NA)\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "concatenated_df = pd.concat([electricity1, electricity2])\n",
    "\n",
    "# Sort by dates\n",
    "concatenated_df.sort_values(by='Date', inplace=True)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "concatenated_df.to_csv('electricity.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "mcx = pd.read_excel('/Users/soumilhooda/Desktop/WD/Data-WD/MCX-Commodity-Trading-Statistics/Messy/trading-statistics-apr-2016.xlsx', skiprows=4)\n",
    "\n",
    "# Rename columns for clarity\n",
    "mcx.columns = ['Year', 'Month', 'Date', 'Useless1', 'Equity Type', 'Commodity Name', 'Useless2', 'Expiry', 'Price Unit', 'C/P Option', 'Strike Price', 'Open Price', 'Useless3', 'Useless4', 'Useless5', 'Useless6', 'Spot Price', 'Useless7', 'Useless8', 'Trading Volume Lots', 'Useless9', 'Lot Size Units', 'Useless10', 'Useless11', 'Useless12', 'Useless13']\n",
    "\n",
    "# Drop columns containing 'Useless' in their name\n",
    "mcx = mcx.drop(columns=mcx.columns[mcx.columns.str.contains('Useless')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to calculate the last day of the month\n",
    "def last_day_of_month(year, month):\n",
    "    if month == 2:  # February\n",
    "        if year % 4 == 0 and (year % 100 != 0 or year % 400 == 0):  # Leap year check\n",
    "            return 29  # Leap year\n",
    "        else:\n",
    "            return 28  # Non-leap year\n",
    "    elif month in [4, 6, 9, 11]:  # Months with 30 days\n",
    "        return 30\n",
    "    else:\n",
    "        return 31  # All other months\n",
    "\n",
    "# Function to process each file\n",
    "def process_file(input_path, output_folder):\n",
    "    try:\n",
    "        # Read the Excel file\n",
    "        mcx = pd.read_excel(input_path, skiprows=4)\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        mcx.columns = ['Year', 'Month', 'Date', 'Useless1', 'Equity Type', 'Commodity Name', 'Useless2', 'Expiry', 'Price Unit', 'C/P Option', 'Strike Price', 'Open Price', 'Useless3', 'Useless4', 'Useless5', 'Useless6', 'Spot Price', 'Useless7', 'Useless8', 'Trading Volume Lots', 'Useless9', 'Lot Size Units', 'Useless10', 'Useless11', 'Useless12', 'Useless13']\n",
    "        \n",
    "        # Drop columns containing 'Useless' in their name\n",
    "        mcx = mcx.drop(columns=mcx.columns[mcx.columns.str.contains('Useless')])\n",
    "        \n",
    "        # Filter rows based on commodity name\n",
    "        mcx = mcx[mcx['Commodity Name'].isin(['CRUDEOIL', 'NATURALGAS', 'COTTON', 'ENRGDEX'])]\n",
    "        \n",
    "        # Save the modified file to the output folder\n",
    "        output_filename = os.path.join(output_folder, os.path.splitext(os.path.basename(input_path))[0] + \".csv\")\n",
    "        mcx.to_csv(output_filename, index=False)\n",
    "        \n",
    "        # print(f\"Processed: {input_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_path}: {e}\")\n",
    "\n",
    "# Input folder containing the files\n",
    "input_folder = '/Users/soumilhooda/Desktop/WD/Data-WD/MCX-Commodity-Trading-Statistics/Messy'\n",
    "\n",
    "# Output folder to save modified files\n",
    "output_folder = '/Users/soumilhooda/Desktop/WD/Data-WD/MCX-Commodity-Trading-Statistics/Cleaned'\n",
    "\n",
    "# Iterate through all files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        process_file(input_path, output_folder)\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "agri = pd.read_csv(\"/Users/soumilhooda/Desktop/WD/Data-WD/Agriculture-Water/Agriculture-Water.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_data(data_filename, weights_filename):\n",
    "    \"\"\"\n",
    "    Processes agricultural data, replaces missing data with 0, and calculates \n",
    "    weighted average rainfall requirements.\n",
    "\n",
    "    Args:\n",
    "        data_filename (str): Path to the CSV file with agricultural data.\n",
    "        weights_filename (str): Path to the CSV file with crop weights.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with keys as (state, month_number) and values as \n",
    "              the weighted average rainfall requirement for that state and month.\n",
    "    \"\"\"\n",
    "\n",
    "    rainfall_requirements = defaultdict(lambda: (0.0, 0.0))  # (total_rainfall, total_weight)\n",
    "\n",
    "    # Load crop weights from the weights file\n",
    "    crop_weights = {}\n",
    "    with open(weights_filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            crop_name, weight = row\n",
    "            crop_weights[crop_name.strip()] = float(weight)\n",
    "\n",
    "    # Process agricultural data\n",
    "    with open(data_filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "\n",
    "        # Extract state names and crop names from headers\n",
    "        states = []\n",
    "        crops = []\n",
    "        for col_header in header[1:]:\n",
    "            if '/' in col_header:\n",
    "                state, crop = col_header.split('/')\n",
    "                states.append(state.strip())\n",
    "                crops.append(crop.strip())\n",
    "            else:\n",
    "                states.append(None)\n",
    "                crops.append(None)\n",
    "\n",
    "        for row in reader:\n",
    "            date, *crop_data = row\n",
    "            month_number = int(date.split('-')[1])\n",
    "\n",
    "            for i, (state, crop) in enumerate(zip(states, crops)):\n",
    "                if state and crop:\n",
    "                    crop_value = crop_data[i] or 0.0  # Replace missing data with 0\n",
    "                    weight = crop_weights.get(crop, 0.0)  # Get weight (0 if not found)\n",
    "\n",
    "                    # Update total rainfall and weight for the state and month\n",
    "                    total_rainfall, total_weight = rainfall_requirements[(state, month_number)]\n",
    "                    rainfall_requirements[(state, month_number)] = (\n",
    "                        total_rainfall + float(crop_value) * weight, \n",
    "                        total_weight + weight\n",
    "                    )\n",
    "\n",
    "    # Calculate weighted averages\n",
    "    for key, (total_rainfall, total_weight) in rainfall_requirements.items():\n",
    "        if total_weight > 0:\n",
    "            rainfall_requirements[key] = total_rainfall / total_weight\n",
    "        else:\n",
    "            rainfall_requirements[key] = 0.0  # Handle cases with no valid crop data\n",
    "\n",
    "    return rainfall_requirements\n",
    "\n",
    "data_filename = \"/Users/soumilhooda/Desktop/WD/Data-WD/Agriculture-Water/Agriculture-Water.csv\"\n",
    "weights_filename = \"/Users/soumilhooda/Desktop/WD/Data-WD/Agriculture-Water/Agriculture-Weights.csv\"\n",
    "rainfall_dict = process_data(data_filename, weights_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agriculture_water = pd.read_csv(\"/Users/soumilhooda/Desktop/WD/Data-WD/Agriculture-Water/Agriculture-Water.csv\")\n",
    "agriculture_weights = pd.read_csv(\"/Users/soumilhooda/Desktop/WD/Data-WD/Agriculture-Water/Agriculture-Weights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_weighted_monthly_rainfall(df_water, df_weights):\n",
    "    \"\"\"\n",
    "    Calculates weighted monthly cumulative rainfall for each state and crop.\n",
    "\n",
    "    Args:\n",
    "        df_water: Dataframe containing daily rainfall data for each state and crop.\n",
    "        df_weights: Dataframe containing weights for each state and crop.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with keys as (state name, month number) and values as weighted monthly rainfall.\n",
    "    \"\"\"\n",
    "\n",
    "    rref = {}\n",
    "    for state_crop in df_water.columns[1:]:  # Skip the 'Date' column\n",
    "        state, crop = state_crop.split(\"/\")\n",
    "        weight = df_weights[state_crop].iloc[0]  # Get weight for the state-crop pair\n",
    "\n",
    "        monthly_data = df_water[[\"Date\", state_crop]].set_index(\"Date\")\n",
    "        monthly_data.index = pd.to_datetime(monthly_data.index)\n",
    "        monthly_data[\"Month\"] = monthly_data.index.month  # Extract month number\n",
    "\n",
    "        monthly_rainfall = monthly_data.groupby(\"Month\")[state_crop].sum() * 1\n",
    "\n",
    "        for month, rainfall in monthly_rainfall.items():\n",
    "            key = (state.upper(), month)\n",
    "            if key not in rref:\n",
    "                rref[key] = rainfall\n",
    "            else:\n",
    "                rref[key] += rainfall\n",
    "\n",
    "    return rref\n",
    "\n",
    "# Example usage:\n",
    "rref = calculate_weighted_monthly_rainfall(agriculture_water, agriculture_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
